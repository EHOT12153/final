Назначение

Локальная LLM используется для пост-анализа результатов нагрузочного тестирования и генерации текстовых отчётов.
Она развёртывается отдельно от тестового стенда и не участвует в генерации нагрузки или сборе метрик.

Архитектурно LLM выступает как внешний аналитический сервис, доступный по HTTP.

Аппаратная конфигурация (фактическая)
Хост: локальная машина (Windows)

ОС: Windows 10 / 11 x64

CPU: 8 логических ядер

RAM: 16 GB

Диск: ≥ 60 GB

GPU: отсутствует или используется частично (не критично)

Данная конфигурация достаточна для запуска LLM класса 7–8B параметров в режиме:

CPU inference

или частичного GPU-offload (если GPU присутствует)

Обоснование выбора конфигурации

16 GB RAM — минимально комфортный объём для моделей ~5 GB с KV-кэшем и контекстом 8k.

8 CPU потоков позволяют выполнять инференс без деградации ОС и параллельной работы Jenkins / Docker.

Размещение LLM на Windows, а не на стенде:

исключает влияние LLM на результаты нагрузочных тестов,

упрощает отладку,

отражает реальный enterprise-паттерн (внешний аналитический сервис).

Установка LM Studio
1. Загрузка и установка

Скачать LM Studio с официального сайта.

Установить стандартным установщиком (без дополнительных компонентов).

Убедиться, что приложение запускается и видит системную память.

Загрузка модели
Используемая модель

Название: meta-llama-3.1-8b-instruct

Формат: GGUF

Размер: ~4.9 GB

Тип: Instruct (оптимален для отчётов и аналитики)

Почему именно она

Хорошо работает на CPU.

Достаточный reasoning для анализа метрик.

Не требует GPU для стабильной работы.

Адекватно держит формат и структуру ответа.

Настройка LM Studio (ключевые параметры)
Режим работы

Модель запускается в режиме OpenAI-compatible server.

Адрес сервера по умолчанию:

http://127.0.0.1:1234

Рекомендуемые параметры (под 16 GB RAM)
Параметр	Значение	Пояснение
Context Length	8192	Достаточно для промпта + метрик
CPU Thread Pool Size	6	Оставляет ресурсы системе
Evaluation Batch Size	512	Баланс скорости и стабильности
Keep Model in Memory	ON	Исключает холодный старт
Try mmap()	ON	Экономия RAM
Flash Attention	ON	Снижение latency
Offload KV Cache	ON	Стабильность на длинных контекстах
GPU Offload	0–8	Опционально, если есть GPU
Seed	Random / OFF	Детерминизм не требуется

Увеличение context length выше 8k не рекомендуется на 16 GB RAM — возможны OOM.

Проверка работоспособности LLM

После запуска модели:

В интерфейсе LM Studio должен быть статус READY.

Сервер должен быть доступен по адресу:

http://127.0.0.1:1234/v1/models


Ожидаемый результат — JSON со списком моделей.

Установка окружения для llm_report.py
Python

Рекомендуемая версия:

Python 3.12+

Проверка:

python --version

Установка зависимостей
pip install -r requirements.txt


(включая openai / httpx / python-docx и др.)

Конфигурация llm_report.py

Скрипт использует OpenAI-совместимый API.

Типовая конфигурация:

LLM_BASE_URL = "http://127.0.0.1:1234/v1"
LLM_MODEL = "meta-llama-3.1-8b-instruct"
LLM_TIMEOUT = 120


API-ключ не требуется, так как сервер локальный.

Взаимодействие со стендом

Jenkins на Ubuntu:

выполняет нагрузочный тест,

формирует артефакты (JTL, метрики, графики).

Jenkins вызывает llm_report.py (через bat/ssh/agent).

llm_report.py:

собирает факты,

отправляет один запрос в LM Studio,

получает текст отчёта,

формирует DOCX.

LLM:

не блокирует пайплайн,

не влияет на расчёты,

может быть отключена без падения теста (fallback-режим).

Отказоустойчивость

Если LM Studio:

не запущен,

не отвечает,

превышен timeout,

llm_report.py:

продолжает работу,

формирует отчёт без LLM,

логирует событие:

[llm] disabled/unavailable -> using local analysis

Ограничения

LLM используется только для текста.

Все метрики считаются до обращения к модели.

Результаты полностью воспроизводимы без LLM.